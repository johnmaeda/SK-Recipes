{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This üìò `.NET Interactive` notebook needs to be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Recipe: üïµÔ∏è Prompt Secrets\n",
    "## üßë‚Äçüç≥ Masterfully cook your prompts to perfection\n",
    "\n",
    "There are plenty of resources available on the Internet to become an expert prompter. Well, that's not really true because LLM AIs are still relatively new and everyone's trying to figure it all out still. Here's three tips to better prompting that might get you started along your journey as a so-called \"prompt engineer.\"\n",
    "\n",
    "## Step 1: Instantiate a üî• kernel so we can start cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 0.9.61.1-preview\"\n",
    "\n",
    "#!import ../config/Settings.cs\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Configuration;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "using Microsoft.SemanticKernel.Orchestration;\n",
    "\n",
    "IKernel kernel = Microsoft.SemanticKernel.Kernel.Builder.Build();\n",
    "\n",
    "// Grab the locally stored credentials from the settings.json file. Name the \"backend\" as \"davinci\" ‚Äî assuming that you're using one of the davinci completion models. \n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "if (useAzureOpenAI)\n",
    "    kernel.Config.AddAzureOpenAITextCompletion(\"davinci\", model, azureEndpoint, apiKey);\n",
    "else\n",
    "    kernel.Config.AddOpenAITextCompletion(\"davinci\", model, apiKey, orgId);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Go through the individual prompting tips in any order that you like\n",
    "\n",
    "Note that we are using a more compact representation for setting up a semantic function and running it all in one code block. It's a different approach, but it has the same outcome as the other method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip #1: Always give more ü•ë context in the prompt\n",
    "\n",
    "The context of your prompt is like the ü•ë fat in a meal. It's rich and indulgent ‚Äî often times it's what makes the meal. LLMs work well when you give as much context as possible. That's usually done by simply giving an example for how something should get done. When you give just one example, that's called \"one shot\" training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "I am someone who puts their time and money where their mouth is.\n",
    "For example: When I say I care about the environment, I donate to environmental causes.\n",
    "For example: When I say {{$input}}\n",
    "\", maxTokens: 100, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync(\"health matters to me\");\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is likely in line with the example, but it doesn't constrain to the idea of \"donating to a cause.\" So instead, you can feed the prompt with more than one example ‚Äî which is called \"few shot\" training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "I am someone who puts their time and money where their mouth is.\n",
    "For example: When I say I care about the environment, I donate to environmental causes.\n",
    "For example: When I say I know that traffic is a problem in my city, I donate to a traffic safety organization.\n",
    "For example: When I say {{$input}}\n",
    "\", maxTokens: 100, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync(\"health matters to me\");\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're likely to see it say `I donate to ....` as the response. The more examples you provide, the more deterministic the output will veer towards. But keep in mind that sometimes if you give it too many examples, it can have the opposite effect.\n",
    "\n",
    "Lastly, you can also just be lazy and give it no example at all. This is called \"zero shot\" training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "Because {{$input}} I need to:\n",
    "    \", maxTokens: 100, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync(\"health matters to me\");\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the response can be pretty amazing. To see a wider range of responses, change the `temperature` parameter to `0.9` or change the `topP` to `0.1` ‚Äî these parameters govern the \"shape\" of what the LLM AI will generate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip #2: Shorten an often-used prompt \n",
    "\n",
    "You can reduce the amount of tokens that get used by finding a similar but shorter version of a prompt That way if you‚Äôre using the prompt a lot, you‚Äôre not using up as many tokens in a cumulative sense. If you're wondering how many tokens will be used by your prompt, the [online tokenizer calculator](https://platform.openai.com/tokenizer) from OpenAI is eminently useful.\n",
    "\n",
    "For example, the prompt:\n",
    "\n",
    "```\n",
    "Take the list of global cities that represent my favorite destinations:\n",
    "\n",
    "START OF LIST\n",
    "Los Angeles, Tokyo, Madagascar, Boston, Phoenix, Paris, Dusseldorf\n",
    "END OF LIST\n",
    "\n",
    "and alphabetize them carefully such that all proper rules of English are used\n",
    "```\n",
    "\n",
    "That prompt would use 60 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "Take the list of global cities that represent my favorite destinations:\n",
    "\n",
    "START OF LIST\n",
    "Los Angeles, Tokyo, Madagascar, Boston, Phoenix, Paris, Dusseldorf\n",
    "END OF LIST\n",
    "\n",
    "and alphabetize them carefully such that all proper rules of English are used\", maxTokens: 100, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(\"60 token prompt ==> \" + result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with the same prompt but made shorter and only using 29 tokens:\n",
    "\n",
    "```\n",
    "Alphabetize:\n",
    "\n",
    "===\n",
    "Los Angeles, Tokyo, Madagascar, Boston, Phoenix, Paris, Dusseldorf\n",
    "===\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "Alphabetize:\n",
    "\n",
    "===\n",
    "Los Angeles, Tokyo, Madagascar, Boston, Phoenix, Paris, Dusseldorf\n",
    "===\n",
    "\", maxTokens: 100, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(\"29 token prompt ==> \" + result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a cost perspective, the shorter prompt would pay off in the long run. Token length is an important consideration not only from the point of view of economics, but also because of fundamental limitations in how many tokens you can use for the model you've selected. We'll go into detail on this point further in the S1E6 notebook that's coming right up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip #3: Go for a two-fer or three-fer\n",
    "\n",
    "Sometimes with one prompt you can get multiple things done. It's like asking a combo-prompt where you've made a request to solve a problem that has three or multiple facets to it. For example, you could execute two prompts in sequence to work on a similar question like, generating a summary of a text and using 77 input tokens: \n",
    "\n",
    "```\n",
    "Analyze the following text to produce a summary for a second grader: \n",
    "\n",
    "I went to the quantum realm to discover a new kind \n",
    "of machine to cure my oldest daughter from a\n",
    "long-standing ailment. It wasn't easy. I had to \n",
    "travel through time and space to reach the realm \n",
    "and collect magical ingredients that could be used \n",
    "to create the medicine.\n",
    "```\n",
    "\n",
    "And then processing a second prompt to generate a list of keywords that will use 75 input tokens:\n",
    "\n",
    "```\n",
    "Analyze the following text to produce an alphabetized list of keywords:\n",
    "\n",
    "I went to the quantum realm to discover a new kind \n",
    "of machine to cure my oldest daughter from a\n",
    "long-standing ailment. It wasn't easy. I had to travel \n",
    "through time and space to reach the realm \n",
    "and collect magical ingredients that could be \n",
    "used to create the medicine.\n",
    "```\n",
    "\n",
    "That's a total of 152 tokens used on two different completion calls to the LLM AI. Instead, both requests can be combined into a single prompt that only takes 87 input tokens:\n",
    "\n",
    "```\n",
    "Analyze the following text to produce: 1. Summary for a second grader, 2. Alphabetized list of keywords. \n",
    "\n",
    "I went to the quantum realm to discover a new \n",
    "kind of machine to cure my oldest daughter from a\n",
    "long-standing ailment. It wasn't easy. I had to \n",
    "travel through time and space to reach the realm \n",
    "and collect magical ingredients that could be \n",
    "used to create the medicine.\n",
    "```\n",
    "\n",
    "Let's try that out and see if it really works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "Analyze the following text to produce: 1. Summary for a second grader, 2. Alphabetized list of keywords, 3. One word summary.\n",
    "\n",
    "I went to the quantum realm to discover a new kind of machine to cure my oldest daughter from a\n",
    "long-standing ailment. It wasn't easy. I had to travel through time and space to reach the realm \n",
    "and collect magical ingredients that could be used to create the medicine.\n",
    "\n",
    "1.\n",
    "\", maxTokens: 150, temperature: 0.4, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(\"87 token prompt ==> \" + result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it produced three results from just one prompt. You can imagine using this technique to generate more results as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip #4: Use \"chain of thought\" prompting\n",
    "\n",
    "Of all the \"tips\" out there, the current fan-favorite is what's called \"chain of thought\" prompting as referenced in [this scientific paper](https://arxiv.org/abs/2201.11903). How does it work? \n",
    "Getting a model to use  ur own human tendency to \"think out loud\" has been shown to produce more \n",
    "accurate outcomes. So when prompting, give an explicit \"thinking out loud\" explanation for how a problem is to be solved with your related reasoning. You'll get a better response because there's more ü•ë context for the model to work with, and ultimately it will provide a better response. Just like when you ask someone to do an important task, and before they run off and do it you ask them to walk you through the steps. Right? It works with people. And it works with LLM AIs too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "I want to eat a mushroom pizza. To make the pizza I need to:\n",
    "\", maxTokens: 100, temperature: 0.1, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response generated should read similar to:\n",
    "\n",
    "1. _Preheat the oven to 375 degrees Fahrenheit._\n",
    "2. _Roll out the pizza dough and place it on a greased baking sheet._\n",
    "3. _Spread a thin layer of tomato sauce over the dough._\n",
    "4. _Top with sliced mushrooms, shredded cheese, and any other desired toppings._\n",
    "5. _Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown._\n",
    "6. _Remove from the oven and let cool_\n",
    "\n",
    "We can ask it for common mistakes for this set of instructions by feeding it back into the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "I want to eat a mushroom pizza. To make the pizza I need to:\n",
    "1. Preheat the oven to 375 degrees Fahrenheit.\n",
    "2. Roll out the pizza dough and place it on a greased baking sheet.\n",
    "3. Spread a thin layer of tomato sauce over the dough.\n",
    "4. Top with sliced mushrooms, shredded cheese, and any other desired toppings.\n",
    "5. Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown.\n",
    "6. Remove from the oven and let cool\n",
    "Common mistakes in these steps include:\n",
    "\", maxTokens: 100, temperature: 0.1, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the list ensues:\n",
    "\n",
    "1. _Not preheating the oven before baking the pizza._\n",
    "2. _Not greasing the baking sheet before placing the dough on it._\n",
    "3. _Not spreading the tomato sauce evenly over the dough._\n",
    "4. _Overloading the pizza with toppings._\n",
    "5. _Not baking the pizza long enough._\n",
    "6. _Not allowing the pizza to cool before serving._\n",
    "\n",
    "Some of the common mistakes are obvious. But some of them aren't -- like step 4 or 5, for instance. We can feed this list back into the prompt and ask it for an improved series of steps to make a pizza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var myInlineSemanticFunction = kernel.CreateSemanticFunction(@\"\n",
    "I want to eat a mushroom pizza. To make the pizza I need to:\n",
    "1. Preheat the oven to 375 degrees Fahrenheit.\n",
    "2. Roll out the pizza dough and place it on a greased baking sheet.\n",
    "3. Spread a thin layer of tomato sauce over the dough.\n",
    "4. Top with sliced mushrooms, shredded cheese, and any other desired toppings.\n",
    "5. Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown.\n",
    "6. Remove from the oven and let cool\n",
    "Common mistakes in these steps include:\n",
    "1. Not preheating the oven before baking the pizza.\n",
    "2. Not greasing the baking sheet before placing the dough on it.\n",
    "3. Not spreading the tomato sauce evenly over the dough.\n",
    "4. Overloading the pizza with toppings.\n",
    "5. Not baking the pizza long enough.\n",
    "6. Not allowing the pizza to cool before serving.\n",
    "Rethinking the instructions for making a pizza so that a perfect pizza can be made every time:\n",
    "\", maxTokens: 200, temperature: 0.3, topP: 1);\n",
    "var result = await myInlineSemanticFunction.InvokeAsync();\n",
    "Console.WriteLine(result);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting pizza instructions read as follows:\n",
    "\n",
    "1. _Preheat the oven to 375 degrees Fahrenheit._\n",
    "2. _Grease a baking sheet and roll out the pizza dough onto it._\n",
    "3. _Spread a thin layer of tomato sauce evenly over the dough._\n",
    "4. _Top the pizza with a moderate amount of sliced mushrooms, shredded cheese, and any other desired toppings._\n",
    "5. _Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown._\n",
    "6. _Allow the pizza to cool for a few minutes before serving._\n",
    "\n",
    "Which are superior to the original set of instructions that were provided. Just for comparison:\n",
    "\n",
    "| Fast thinking | Slow thinking (Chain of thought) | Difference\n",
    "|---|---|---|\n",
    "| _Preheat the oven to 375 degrees Fahrenheit._ | _Preheat the oven to 375 degrees Fahrenheit._ | Same |\n",
    "| _Roll out the pizza dough and place it on a greased baking sheet._ | _Grease a baking sheet and roll out the pizza dough onto it._ | Emphasizes greasing |\n",
    "| _Spread a thin layer of tomato sauce over the dough._ | _Spread a thin layer of tomato sauce evenly over the dough._ | Emphasizes evenness |\n",
    "| _Top with sliced mushrooms, shredded cheese, and any other desired toppings._ | _Top the pizza with a moderate amount of sliced mushrooms, shredded cheese, and any other desired toppings._ | Emphasizes moderation |\n",
    "| _Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown._ | _Bake for 15-20 minutes, or until the cheese is melted and the crust is golden brown._ | Same |\n",
    "| _Remove from the oven and let cool_ | _Allow the pizza to cool for a few minutes before serving._ | Recommends time |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[You're all done! Visit the main GitHub repo to check out what's new ‚Äî because LLM AI is changing ever-so-rapidly!](https://aka.ms/sk/repo)\n",
    "\n",
    "Or, stay a longer while and change the objective of a chain of thought prompt to see how your results compare when you get the LLM AI to \"think out loud\" a bit."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
