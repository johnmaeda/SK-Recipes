{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This üìò `.NET Interactive` notebook needs to be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Recipe: üïµÔ∏è Test your prompts\n",
    "## üß™ Non-deterministic computation requires testing\n",
    "\n",
    "Because the output from LLM AI is unpredictable in nature, you're going to want to enforce some degree of reliability that it will behave a similar way each time. \n",
    "\n",
    "## Step 1: Instantiate a üî• kernel so we can start cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 0.9.61.1-preview\"\n",
    "\n",
    "#!import ../config/Settings.cs\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Configuration;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "using Microsoft.SemanticKernel.Orchestration;\n",
    "\n",
    "IKernel kernel = Microsoft.SemanticKernel.Kernel.Builder.Build();\n",
    "\n",
    "// Grab the locally stored credentials from the settings.json file. Name the \"backend\" as \"davinci\" ‚Äî assuming that you're using one of the davinci completion models. \n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "if (useAzureOpenAI)\n",
    "    kernel.Config.AddAzureOpenAITextCompletion(\"davinci\", model, azureEndpoint, apiKey);\n",
    "else\n",
    "    kernel.Config.AddOpenAITextCompletion(\"davinci\", model, apiKey, orgId);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We're going to make one semantic function and run it at two different temperatures\n",
    "\n",
    "Different temperatures result in different outcomes ‚Äî we can illustrate that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "\n",
    "string myFun = \"\"\"\n",
    "{{$input}}\n",
    "Define the term above in less than five words.\n",
    "\"\"\";\n",
    "\n",
    "var promptConfigA = new PromptTemplateConfig\n",
    "{\n",
    "    Completion =\n",
    "    {\n",
    "        Temperature = 0.2, \n",
    "        TopP = 0.1\n",
    "    }\n",
    "};\n",
    "\n",
    "var promptConfigB = new PromptTemplateConfig\n",
    "{\n",
    "    Completion =\n",
    "    {\n",
    "       Temperature = 1,\n",
    "       TopP = 1.0\n",
    "    }\n",
    "};\n",
    "\n",
    "var functionConfigA = new SemanticFunctionConfig(promptConfigA, new PromptTemplate( myFun, promptConfigA, kernel ));\n",
    "var functionConfigB = new SemanticFunctionConfig(promptConfigB, new PromptTemplate( myFun, promptConfigB, kernel ));\n",
    "\n",
    "var myFunCold = kernel.RegisterSemanticFunction(\"MyTest\", \"A\", functionConfigA);\n",
    "var myFunHot = kernel.RegisterSemanticFunction(\"MyTest\", \"B\", functionConfigB);\n",
    "\n",
    "Console.WriteLine(\"A semantic function has been registered with two configurations.\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run these two functions over multiple iterations to see how they differ, or not. Note that our configurations are as follows:\n",
    "\n",
    "### Configuration A\n",
    "\n",
    "* Temperature = 0.2 <-- ranges from 0 (low randomness) to 1 (high randomness)\n",
    "* TopP = 0.1 <-- ranges from 0 (low variability) to 1 (high variability)\n",
    "\n",
    "### Configuration B\n",
    "\n",
    "* Temperature = 1 <-- ranges from 0 (low randomness) to 1 (high randomness)\n",
    "* TopP = 1.0 <-- ranges from 0 (low variability) to 1 (high variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var input = \"AI\";\n",
    "var coldResponses = \"\";\n",
    "var hotResponses = \"\";\n",
    "for(var i = 0; i < 5; i++) {\n",
    "   var coldResult = await kernel.RunAsync(input, myFunCold);\n",
    "   coldResponses+=coldResult.ToString().Trim() + \"\\n\";\n",
    "   var hotResult = await kernel.RunAsync(input, myFunHot);\n",
    "   hotResponses+=hotResult.ToString().Trim() + \"\\n\";\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"COLD responses:\\n\\n{coldResponses}\");\n",
    "Console.WriteLine($\"\\nHOT responses:\\n\\n{hotResponses}\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let's get freezing cold to aim towards a higher degree of determinism in outputs.\n",
    "\n",
    "You should note that the COLD responses don't deviate so much; whereas the HOT responses do. Another set of parameters to consider that increase variability are `PresencePenalty` and `FrequencyPenalty`. As the official definition goes:\n",
    "\n",
    "* `PresencePenalty`: \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\"\n",
    "* `FrequencyPenalty`: \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\"\n",
    "\n",
    "So if you have the LLM AI generating a longer piece of text, you can either have it try to use new words (instead of repeating old ones) either by nudging the usage frequency or count of the word (i.e. \"presence\") with positive number settings. Or when using negative number settings it will be happy repeating itself by using the same words over and over. \n",
    "\n",
    "The benefit of using the same words repetitively is that the LLM AI won't veer off course so easily. That said, you may think it's a bit ... boring. But sometimes boring is what you want when you'd like the same answer to always get generated. Let's try this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "\n",
    "string myFunNext = \"\"\"\n",
    "Term: {{$input}}\n",
    "A fifteen word story about the term:\n",
    "\"\"\";\n",
    "\n",
    "var promptConfigC = new PromptTemplateConfig\n",
    "{\n",
    "    Completion =\n",
    "    {\n",
    "        Temperature = 0.2, \n",
    "        TopP = 0.1,\n",
    "        PresencePenalty = 2,\n",
    "        FrequencyPenalty = 2\n",
    "    }\n",
    "};\n",
    "\n",
    "var promptConfigD = new PromptTemplateConfig\n",
    "{\n",
    "    Completion =\n",
    "    {\n",
    "        Temperature = 1,\n",
    "        TopP = 1.0,\n",
    "        PresencePenalty = -2,\n",
    "        FrequencyPenalty = -2\n",
    "    }\n",
    "};\n",
    "\n",
    "var functionConfigC = new SemanticFunctionConfig(promptConfigC, new PromptTemplate( myFunNext, promptConfigC, kernel ));\n",
    "var functionConfigD = new SemanticFunctionConfig(promptConfigD, new PromptTemplate( myFunNext, promptConfigD, kernel ));\n",
    "\n",
    "var myFunColder = kernel.RegisterSemanticFunction(\"MyTest\", \"C\", functionConfigC);\n",
    "var myFunHotter = kernel.RegisterSemanticFunction(\"MyTest\", \"D\", functionConfigD);\n",
    "\n",
    "Console.WriteLine(\"A semantic function has been registered with two more EXTREME configurations.\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run them to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var input = \"AI\";\n",
    "var colderResponses = \"\";\n",
    "var hotterResponses = \"\";\n",
    "for(var i = 0; i < 3; i++) {\n",
    "   var colderResult = await kernel.RunAsync(input, myFunColder);\n",
    "   colderResponses+=colderResult.ToString().Trim() + \"\\n\";\n",
    "   var hotterResult = await kernel.RunAsync(input, myFunHotter);\n",
    "   hotterResponses+=hotterResult.ToString().Trim() + \"\\n\";\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"COLDER responses:\\n\\n{colderResponses}\");\n",
    "Console.WriteLine($\"\\nHOTTER responses:\\n\\n{hotterResponses}\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you understand how to \"tamp down\" the LLM AI's degree of creativity with a COLDER approach. And also how to unleash its freedom of expression with a HOTTER approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Just one more thing ‚Äî let's use a different model\n",
    "\n",
    "Changing the model will also change the result. Let's see that up close by comparing what the output of `text-davinci-003` will be compared with `text-davinci-002` (the older model). We'll start by making a second kernel that uses `text-davinci-002` called `kernel2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 0.9.61.1-preview\"\n",
    "\n",
    "#!import ../config/Settings.cs\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.KernelExtensions;\n",
    "using System.IO;\n",
    "using Microsoft.SemanticKernel.Configuration;\n",
    "using Microsoft.SemanticKernel.SemanticFunctions;\n",
    "using Microsoft.SemanticKernel.Orchestration;\n",
    "\n",
    "IKernel kernel2 = Microsoft.SemanticKernel.Kernel.Builder.Build();\n",
    "\n",
    "// Grab the locally stored credentials from the settings.json file. Name the \"backend\" as \"davinci\" ‚Äî assuming that you're using one of the davinci completion models. \n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "if (useAzureOpenAI)\n",
    "    kernel2.Config.AddAzureOpenAITextCompletion(\"davinci\", \"text-davinci-002\", azureEndpoint, apiKey);\n",
    "else\n",
    "    kernel2.Config.AddOpenAITextCompletion(\"davinci\", \"text-davinci-002\", apiKey, orgId);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll run the same HOTTER vs COLDER scenario with `text-davinci-002` in charge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var colderResponses2 = \"\";\n",
    "var hotterResponses2 = \"\";\n",
    "for(var i = 0; i < 3; i++) {\n",
    "   var colderResult2 = await kernel2.RunAsync(input, myFunColder);\n",
    "   colderResponses2+=colderResult2.ToString().Trim() + \"\\n\";\n",
    "   var hotterResult2 = await kernel.RunAsync(input, myFunHotter);\n",
    "   hotterResponses2+=hotterResult2.ToString().Trim() + \"\\n\";\n",
    "}\n",
    "\n",
    "Console.WriteLine($\"COLDER responses (text-davinci-002):\\n\\n{colderResponses2}\");\n",
    "Console.WriteLine($\"\\nHOTTER responses (text-davinci-002):\\n\\n{hotterResponses2}\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully, the COLDER responses should be similar or the same. The HOTTER ones, however, will be different. You'll notice that `text-davinci-003` felt less robot-like compared with `text-davinci-002.` But don't forget that each model has different cost structures -- so you can choose the model that matches your quality needs and economics by learning how to tune these basic parameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[You're all done! Visit the main GitHub repo to check out what's new ‚Äî because LLM AI is changing ever-so-rapidly!](https://aka.ms/sk/repo)\n",
    "\n",
    "Or, stay a longer while and modify the various parameters to get a better feel from them. Just be careful how many times you keep calling the models in a loop ‚Äî because the $$$ can quickly add up!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
